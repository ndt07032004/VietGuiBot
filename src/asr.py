import pyaudio
import numpy as np
from typing import Optional
import asyncio
from src.logger import logger
import whisper
import webrtcvad
import torch


async def init_asr(config: dict):
    """
    Khá»Ÿi táº¡o model ASR real-time báº±ng Whisper + VAD.
    Tá»± Ä‘á»™ng chá»n GPU náº¿u cÃ³, náº¿u khÃ´ng fallback CPU.
    """
    try:
        model_name = config['asr'].get('model', 'small')  # base | small | medium | large

        # ðŸ”¥ Æ¯u tiÃªn GPU náº¿u cÃ³
        if torch.cuda.is_available():
            device = "cuda"
        else:
            device = "cpu"

        model = whisper.load_model(model_name, device=device)
        logger.info(f"Whisper model {model_name} loaded on {device}")
        return model
    except Exception as e:
        logger.error(f"Failed to load Whisper ASR: {e}")
        raise


async def transcribe_audio(model, sample_rate: int = 16000) -> Optional[str]:
    """
    Chuyá»ƒn Ä‘á»•i audio thÃ nh text tiáº¿ng Viá»‡t real-time.
    """
    try:
        audio_data = await record_audio(sample_rate)
        if audio_data is None:
            return None

        # Whisper cáº§n float32 PCM
        result = model.transcribe(audio_data, language="vi")
        text = result.get("text", "").strip()
        logger.info(f"Transcription: {text}")
        return text
    except Exception as e:
        logger.error(f"Transcription error: {e}")
        return None


async def record_audio(sample_rate: int = 16000) -> Optional[np.ndarray]:
    """
    Ghi Ã¢m tá»« microphone vá»›i VAD thá»§ cÃ´ng (WebRTC).
    """
    p = pyaudio.PyAudio()
    stream = None
    try:
        stream = p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=sample_rate,
            input=True,
            frames_per_buffer=512
        )
        frames = []
        vad = webrtcvad.Vad(mode=3)  # 0-3 (nháº¡y tÄƒng dáº§n)
        logger.info("ðŸŽ™ï¸ Báº¯t Ä‘áº§u ghi Ã¢m... (tá»± dá»«ng khi im láº·ng)")

        while True:
            data = await asyncio.to_thread(stream.read, 512)
            if vad.is_speech(data, sample_rate):
                frames.append(np.frombuffer(data, dtype=np.int16))
            else:
                if len(frames) > 0:
                    break
            # Giá»›i háº¡n max 15s Ä‘á»ƒ trÃ¡nh káº¹t
            if len(frames) * 512 / sample_rate > 15:
                break

        stream.stop_stream()
        stream.close()
        p.terminate()

        if not frames:
            return None

        audio = np.concatenate(frames).astype(np.float32) / 32768.0
        return audio
    except Exception as e:
        logger.error(f"Recording error: {e}")
        if stream:
            stream.stop_stream()
            stream.close()
        p.terminate()
        return None